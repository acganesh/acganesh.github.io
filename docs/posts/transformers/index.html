<!DOCTYPE html>
<html lang="en-us"><head>
<title>GPT in words and code - Adi Ganesh</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
<meta name="description"
    content="Notes on transformers / LLMs from the ground up. ">
<link rel="canonical" href="https://acganesh.github.io/posts/transformers/" />


<link rel="icon" href="https://acganesh.github.io/favicon.ico" />


<link rel="apple-touch-icon" href="https://acganesh.github.io/touch-icon.png" />

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/modern-normalize/1.1.0/modern-normalize.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />










<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link rel="preload" as="style"
      href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Noto+Emoji&display=swap" />
<link rel="stylesheet"
      href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Noto+Emoji&display=swap"
      media="print" onload="this.media='all'" />
<noscript>
<link rel="stylesheet"
      href="https://fonts.googleapis.com/css?family=Noto+Serif+SC&display=swap" />
</noscript>






<link rel="stylesheet" href="/css/hugo-tufte.min.css">

<link rel="stylesheet" href="/css/hugo-tufte-override.css">

</head>
<body >

<article id="main">
  <section>
<h1 class="content-title">GPT in words and code</h1></section>

  

  <section><p>I find that the best way to understand how machine learning papers work is to write the code for a model forward pass.  If you can load the weights from a pre-trained model and get the same outputs from a single model inference, you can be pretty confident that you&rsquo;ve re-implemented all of the details from a model.  The advantages of doing this are:</p>
<ul>
<li>Does not require any training, which can be time-consuming and expensive.</li>
<li>Can test model outputs layer-by-layer to validate the correctness of different components.</li>
<li>Get a satisfying payoff at the end with a working model, and develop an understanding of the model that is more detailed than what is found in the paper.</li>
</ul>
<p>This is the strategy adopted in Jay Mody&rsquo;s <a href="https://github.com/jaymody/picoGPT">picoGPT</a> and Andrej Karpathy&rsquo;s <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a>.</p>
<p>For a good exercise to replicate GPT-inference, I would recommend reimplementing the <code>gpt2</code> function in the picoGPT repo above, located <a href="https://github.com/jaymody/picoGPT/blob/main/gpt2.py#L90C20-L90C20">here</a>.  picoGPT makes this especially easy because the weight loading code is already written and you can just write the forward pass in NumPy. A link to my implementation can be found <a href="https://github.com/acganesh/picoGPT">here</a>.</p>
<h2 id="model-architecture">
Model architecture
<a href="#model-architecture" class="heading-anchor">#</a>
</h2>
<p>Here I will break down the GPT architecture into its components.  Here I will focus on GPT-2, since the weights are publicly available.  GPT-3 has a very similar architecture but is massively scaled up.</p>
<p>GPT2 can be implemented in the following pseudocode:</p>
<pre tabindex="0"><code>def gpt(input_string):
  input_tokens = tokenize(input_string)
  x = wte[input_tokens] + wpe[range(len(input_tokens))]
  for transformer_block in transformer_blocks:
    x = transformer_block(x) 
  x = layer_norm(x)
  return x @ wte.T
</code></pre><p>In the following sections we will break down each piece.</p>
<h3 id="1-byte-pair-embedding-tokenizer">
1) Byte-Pair Embedding Tokenizer
<a href="#1-byte-pair-embedding-tokenizer" class="heading-anchor">#</a>
</h3>
<p><code>$\text{String} -&gt; \text{List[Integer]}$</code></p>
<p>The first step is to convert words to numbers using a tokenizer.  GPT uses <a href="https://huggingface.co/docs/transformers/tokenizer_summary#bytepair-encoding-bpe">byte-pair encoding</a> (BPE).  In BPE, the most common words are mapped to single tokens while less common words will be broken down into chunks and mapped to multiple tokens.</p>
<p>OpenAI&rsquo;s <a href="https://platform.openai.com/tokenizer">Tokenizer</a> tool shows how different sentences will be broken into tokens under BPE.  In this example, most words get mapped to a single token, except for &ldquo;Sylvester,&rdquo; which gets chunked into three tokens: <code>Sy</code>, <code>lves</code>, and <code>ter</code>.</p>
<p>TODO: Fix image.
<img src="./img/transformers-tokenizer.png" alt="Tokenizer"></p>
<h3 id="21-word-embeddings">
2.1) Word Embeddings
<a href="#21-word-embeddings" class="heading-anchor">#</a>
</h3>
<p>We start by embedding each token which is done with a lookup</p>
<pre tabindex="0"><code>wte[input_tokens].
</code></pre><p>This gives us a tensor of shape <code>$n_{tokens} \times n_{embed}$</code>.  For GPT-2, <code>$n_{embed} = 1600$</code>.</p>
<h3 id="22-positional-encodings">
2.2) Positional Encodings
<a href="#22-positional-encodings" class="heading-anchor">#</a>
</h3>
<p>Transformers are invariant to the order of inputs, so we need to tell the model which position each word is in.  We grab positional embeddings with a similar lookup:</p>
<pre tabindex="0"><code>wpe[range(len(inputs))]
</code></pre><p>This gives us another tensor of shape <code>$n_{tokens} \times n_{embed}$</code>.</p>
<h3 id="23-sum">
2.3) Sum
<a href="#23-sum" class="heading-anchor">#</a>
</h3>
<p>Now we simply sum of the two tensors from before to get a single tensor of shape <code>$n_{tokens} \times n_{embed}$</code>.</p>
<pre tabindex="0"><code>x = wte[inputs] + wpe[range(len(inputs))]
</code></pre><h3 id="3-transformer-block">
3) Transformer Block
<a href="#3-transformer-block" class="heading-anchor">#</a>
</h3>
<p>The transformer block can be expressed as the following operation:</p>
<pre tabindex="0"><code>def transformer_block(x):
  x = x + MultiHeadAttention(LayerNorm(x))
  x = x = FFN(LayerNorm(x))
  return x
</code></pre><h3 id="31-attention">
3.1) Attention
<a href="#31-attention" class="heading-anchor">#</a>
</h3>
<p>We will start by discussing single-head attention.  We define the <em>attention</em> operation as follows:
<code>$$ \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax} \left ( \frac{\mathbf{QK^T}}{\sqrt{d_k}} \right ) \mathbf{V}. $$</code></p>
<p>Here, <code>$\mathbf{Q}, \mathbf{K}, \mathbf{V}$</code> are obtained from a linear layer on the input tensor.</p>
<p>In code, this looks like this:</p>
<pre tabindex="0"><code>causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10

def attention(q, k, v, mask):
    # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -&gt; [n_q, d_v]
    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v
</code></pre><p>Here, the causal mask prevents a tokens from attending to future tokens - in the context of language modeling, this is necessary since we will see each word stream in one by one.</p>
<p>Intuitively, <code>$\mathbf{Q} \mathbf{K}^T$</code> will result in an &ldquo;importance&rdquo; matrix that returns the importance of each token to each other token.  We then divide this by <code>$\sqrt{d_k}$</code> and then pass this through a softmax.  Finally, we multiply this by the <code>$\mathbf{V}$</code> matrix, which represents the importance of each token.</p>
<h3 id="32-multiheadattention">
3.2) MultiHeadAttention
<a href="#32-multiheadattention" class="heading-anchor">#</a>
</h3>
<p>MultiHeadAttention is a simple extension of single-head attention.  Here, we will just redo the above operation several times with a different learnd <code>$\mathbf{Q}, \mathbf{K}$ and </code>$\mathbf{V}$` matrices.  We will then concatenate the result of each attention head together, which is then multiplied by a linear projection.</p>
<p>In code, this looks like this:</p>
<pre tabindex="0"><code>def multi_head_attention(x, c_attn, c_proj, n_head):
    x = linear(x,
               w=c_attn[&#39;w&#39;],
               b=c_attn[&#39;b&#39;])

    qkv = np.split(x, 3, axis=-1)

    qkv_heads = []
    for elt in qkv:
        qkv_head_split = np.split(elt, n_head, axis=-1)
        qkv_heads.append(qkv_head_split)

    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10

    out_heads = []
    for q, k, v in zip(*qkv_heads):
        x = attention(q, k, v, causal_mask)
        out_heads.append(x)
    
    x = np.hstack(out_heads)

    x = linear(x,
               w=c_proj[&#39;w&#39;],
               b=c_proj[&#39;b&#39;])

    return x
</code></pre><h3 id="4-ffn">
4) FFN
<a href="#4-ffn" class="heading-anchor">#</a>
</h3>
<p>The rest of the transformer block is quite simple.  The FFN block looks like this:</p>
<pre tabindex="0"><code>def ffn(x):
  x = linear(x) # project up
  x = gelu(x)
  x = linear(x) # project down
</code></pre><p>The GELU is an activation function defined in <a href="https://arxiv.org/abs/1606.08415">this paper</a>, defined as <code>$x \Phi(x)$</code>, where <code>$\Phi(x)$</code> is the standard Gaussian CDF function.</p>
<p>We will call the FFN block in the transformer block with a skip connection as follows:</p>
<pre tabindex="0"><code>x = x + ffn(x)
</code></pre><h3 id="5-layernorm-and-decode">
5) LayerNorm and Decode
<a href="#5-layernorm-and-decode" class="heading-anchor">#</a>
</h3>
<p>Before decoding words, we will run a last iteration of LayerNorm as follows:</p>
<pre tabindex="0"><code>x = x + layer_norm(x)
</code></pre><p>At the end, we have a big word embedding matrix.  We decode by multiplying by the transpose of <code>$W_E$</code> to get back to tokens:</p>
<pre tabindex="0"><code>x = x @ wte.T
</code></pre><h3 id="demo">
Demo
<a href="#demo" class="heading-anchor">#</a>
</h3>
<p>And that&rsquo;s it!  For a demo, check out my repo. <a href="https://github.com/acganesh/tinyGPT/">https://github.com/acganesh/tinyGPT/</a></p>
</section>
  <section><footer class="page-footer">
<hr />

<div class="previous-post" style="display:inline-block;">
  
  <a class="link-reverse" href="https://acganesh.github.io/posts/2019-07-13-polya-burnside/?ref=footer">« Pólya-Burnside enumeration in combinatorics</a>
  
</div>

<div class="next-post", style="display:inline-block;float:right;">
  
</div>

<ul class="page-footer-menu">
  
  

  

  

  

  

  

  

  

  

  

  

  
  
  
</ul>





</footer>
</section>
  <section><nav class="menu">
    <ul>
    
        <li><a href="/">Home</a></li>
    
        <li><a href="/posts/">Blog</a></li>
    
        <li><a href="/about/">About</a></li>
    
    </ul>
</nav>
</section>
</article>







  <script>(function(){var e,t,n,s=document.getElementsByTagName("code");for(n=0;n<s.length;){if(t=s[n],t.parentNode.tagName!=="PRE"&&t.childElementCount===0&&!t.classList.contains("nolatex")&&(e=t.textContent,/^\$[^$]/.test(e)&&/[^$]\$$/.test(e)&&(e=e.replace(/^\$/,"\\(").replace(/\$$/,"\\)"),t.textContent=e),/^\\\((.|\s)+\\\)$/.test(e)||/^\\\[(.|\s)+\\\]$/.test(e)||/^\$(.|\s)+\$$/.test(e)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(e))){t.outerHTML=t.innerHTML;continue}n++}})()</script>







<script type="text/javascript" async
	  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
            macros: {
              "\\eqref": "\\href{###1}{(\\text{#1})}",
              "\\ref": "\\href{###1}{\\text{#1}}",
              "\\label": "\\htmlId{#1}{}"
            }
        });
    });
</script>



</body>

</html>
