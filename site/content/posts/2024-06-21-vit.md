---
layout: post
title: Vision transfomers
date: 2024-06-21
math: true
description: Explaining how vision transformers work from the ground up
draft: true
---

...

# Introduction

The Vision Transformer can be broken down into 5 components:

- Patchify and embed the image.
- Add positional encodings
- Vanilla transformer
- Pooling
- To latent
- MLP head

## Patchify the image

We will start by patchifying the image.  This means dividing an $n \times n$ image into chunks of size $k \times k$, and embedding each chunk with a linear layer.  This lets us model the image as a "sequence of patches" and apply the usual toolbox of transformers to them.

We will prepend a learnable "cls" token to the input, following the technique applied in BERT.

## Add positional encodings

Since transformers are invariant to the order in which the input is presented to them, we need to add positional encodings to the patch embeddings.  There are various options, but the Vision Transformers paper uses learnable positional encodings.

## Transformer

At this stage, we will use a single transformer encoding to encode the sequence.

## Pooling and MLP

Finally, we will apply optional pooling followed by an MLP.

## Inference Code

The full inference code for the vision transformer can be found here.  This implementation is adapted from Phil Wang's excellent repo, with some modifications to let us load a pretrained model and show that the outputs are equivalent.

