<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Adi Ganesh</title>
    <link>https://acganesh.github.io/posts/</link>
    <description>Recent content in Posts on Adi Ganesh</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Nov 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://acganesh.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Speculative decoding for LLM inference</title>
      <link>https://acganesh.github.io/posts/speculative_decoding/</link>
      <pubDate>Tue, 07 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://acganesh.github.io/posts/speculative_decoding/</guid>
      <description>Speculative decoding is a neat trick that provides significant speedups to LLM inference. This post was originally inspired by this Karpathy tweet &amp;ndash; I will aim to discuss how the algorithm works in more detail and prove its correctness.
The idea is based on the following facts:
LLM inference for batch size $k$ takes approximately the same time as inference for batch size $1$, for surprisingly large values of $k$. In particular, for low batch sizes, LLMs are bound by memory-bandwidth.</description>
    </item>
    
    <item>
      <title>My local LaTeX workflow</title>
      <link>https://acganesh.github.io/posts/2023-11-04-latex/</link>
      <pubDate>Sat, 04 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://acganesh.github.io/posts/2023-11-04-latex/</guid>
      <description>In this post, I&amp;rsquo;ll document the workflow that I use to compose LaTeX documents locally on Mac OS. Back in college I used this workflow to live-TeX lecture notes.
Here&amp;rsquo;s an example that shows it all together:
Personal stylesheet # One of the most basic but powerful features of LaTeX is being able to define your own macros in a stylesheet. I recommend defining your own, which you can then import with \usepackage{stylesheetname}.</description>
    </item>
    
    <item>
      <title>GPT in words and code</title>
      <link>https://acganesh.github.io/posts/transformers/</link>
      <pubDate>Sun, 20 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://acganesh.github.io/posts/transformers/</guid>
      <description>I find that the best way to understand how machine learning papers work is to write the code for a model forward pass. If you can load the weights from a pre-trained model and get the same outputs from a single model inference, you can be pretty confident that you&amp;rsquo;ve re-implemented all of the details from a model. The advantages of doing this are:
Does not require any training, which can be time-consuming and expensive.</description>
    </item>
    
    <item>
      <title>The modern algorithmic toolbox</title>
      <link>https://acganesh.github.io/posts/2019-07-13-modern-algorithmic-toolbox/</link>
      <pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://acganesh.github.io/posts/2019-07-13-modern-algorithmic-toolbox/</guid>
      <description>$$ \newcommand{\mbf}{\mathbf} \newcommand{\RR}{\mathbb{R}} \newcommand{\CC}{\mathbb{C}} \newcommand{\la}{\langle} \newcommand{\ra}{\rangle} \DeclareMathOperator{\rank}{rank} \DeclareMathOperator{\argmax}{argmax} $$
The modern algorithmic toolbox # These notes are based on an influential course I took at Stanford, CS168: The Modern Algorithmic Toolbox, taught by Greg Valiant in Spring 2018.
I found it to be my favorite technical class at Stanford, as I think it&amp;rsquo;s a versatile grab-bag of ideas that can be applied to many different domains.
Modern hashing # Motivating problem.</description>
    </item>
    
    <item>
      <title>The principle of maximum entropy</title>
      <link>https://acganesh.github.io/posts/2019-01-01-maximum-entropy/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://acganesh.github.io/posts/2019-01-01-maximum-entropy/</guid>
      <description>Introduction: what is entropy? # $$ \newcommand{\EE}{\mathbb{E}} \newcommand{\RR}{\mathbb{R}} \newcommand{\mbf}{\mathbf} \DeclareMathOperator{\argmax}{argmax} $$
Intuitively, the notion of entropy defines a measure of &amp;ldquo;disorder&amp;rdquo; or &amp;ldquo;expected surprise&amp;rdquo; given a probability distribution. As described by Shannon in is 1948 article, the entropy can be defined as follows Shannon, Claude Elwood. &amp;ldquo;A mathematical theory of communication.&amp;rdquo; Bell system technical journal 27.3 (1948): 379-423. .
Let $X$ be a discrete random variable on a space $\mathcal{X}$ with probability mass function $\mathbf{p}(x)$.</description>
    </item>
    
    <item>
      <title>Pólya-Burnside enumeration in combinatorics</title>
      <link>https://acganesh.github.io/posts/2018-12-15-polya-burnside/</link>
      <pubDate>Sat, 15 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://acganesh.github.io/posts/2018-12-15-polya-burnside/</guid>
      <description>$$ \newcommand{\mat}[1]{\begin{pmatrix}#1\end{pmatrix}} \DeclareMathOperator{\Stab}{Stab} \DeclareMathOperator{\Fix}{Fix} \DeclareMathOperator{\Orb}{Orb} $$
Pólya-Burnside enumeration in combinatorics # A class of problems # The following problem in chemistry is historically significant, as G. Pólya originally popularized his theory through applications in chemical enumeration. How many different chemical compounds can be made by attaching $H$, $CH_3$, or $OH$ radicals to each of the carbon atoms in the benzene ring pictured below?
Here are other problems that can be approached using Pólya-Burnside.</description>
    </item>
    
    <item>
      <title>The Meissel-Lehmer algorithm</title>
      <link>https://acganesh.github.io/posts/2016-12-23-prime-counting/</link>
      <pubDate>Fri, 23 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://acganesh.github.io/posts/2016-12-23-prime-counting/</guid>
      <description>Introduction # Computing $\pi(x)$, the number of primes $p \leq x$ is a classic problem in algorithmic number theory. The prime number theorem describes the asymptotic growth of this function, and states that
$$ \lim_{x \to \infty} \left. \pi(x) \middle/ \frac{x}{\ln x} \right. = 1. $$
The theorem was independently conjectured by Legendre and Gauss, and proved by Hadamard and de la Vallée Poussin&amp;rsquo;s around 100 years later in 1896.</description>
    </item>
    
  </channel>
</rss>
